{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5, DATSCI W261\n",
    "Team: Kuan Lin, Alejandro J. Rojas, Ricardo Barrera<br/>\n",
    "Emails: kuanlin@ischool.berkeley.edu, ale@ischool.berkeley.edu, ricardofrank@ischool.berkeley.edu<br/>\n",
    "Time of Initial Submission: 8:00 AM PST, Thursday, Feb 18, 2016<br/>\n",
    "W261-1, Spring 2016 Week 6 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data warehouse is an integrated database often used to store both current and historical data for the purpose of reporting and analytics.\n",
    "\n",
    "A Star Schema is a special case of snow-flake schema where there has a fact table at the center surrounded by dimension tables that are referenced to the fact table with foriegn keys.  The star schema has a relaxed normalization rule compared to other types of schemas and therefore can be easier to implement and query for small-scale information systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "In what form does ML consume data?\n",
    "Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3NF (3rd normal form) must first satisfy 1NF and 2NF, which includes:\n",
    "* Eliminate repeating groups in individual tables.\n",
    "* Create a separate table for each set of related data.\n",
    "* Identify each set of related data with a primary key\n",
    "* no non-prime attribute is dependent on any proper subset of any candidate key of the table\n",
    "\n",
    "In addition, the schema must also satisfy:\n",
    "* Every non-prime attribute of a table is non-transitively dependent on every key of a table\n",
    "\n",
    "In instance-based (such as K-mean) and supervised ML algorithms, the algorithm must be able to access all attributes (that made through feature selections) on each data point, and therefore many ML algorithms need a dense matrix of data rather than a normaled form.  Use a denormalized log file allows ML algorithm to parse out all required attributes of the data which satisfies the aforementioned requiredments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right<br/>\n",
    "(2) Right joining Table Left with Table Right<br/>\n",
    "(3) Inner joining Table Left with Table Right<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Since the url table is much smaller, it is better to store the url table in memory as a hashtable and perform the join as the right table.  Therefore the better option is to use the visit-info table on the left, and use the url-table as the memory-backed hash table on the right.  Also, it is possible that the url is not documented, and therefore option 1 (left join) is a prefered solution so that we don't have to drop visit data in the case of unmatched url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, reproduced the needed tables from hw4\n",
    "\n",
    "# make the reformated table for visit info:\n",
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'reformated_webdata.csv'\n",
    "\n",
    "writer = open(output_file, 'w')\n",
    "current_visitor = None\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '' or (not line.startswith(\"C,\") and not line.startswith(\"V\")): continue\n",
    "    lineArr = line.split(',')\n",
    "    if lineArr[0] == 'C':\n",
    "        current_visitor = lineArr[2]\n",
    "    elif current_visitor != None:\n",
    "        writer.write(line + ',C,' + current_visitor + '\\n')\n",
    "writer.close()\n",
    "\n",
    "# make the url table:\n",
    "# need to first extract out the urls, will be used to lookup url in reducer\n",
    "import csv\n",
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'url_data.csv'\n",
    "writer = open(output_file, 'w')\n",
    "\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line.startswith('A,'):\n",
    "        lineArr = list(csv.reader(line.splitlines(), delimiter=',', quotechar='\"'))[0]\n",
    "        writer.write(','.join([lineArr[1], lineArr[4].replace('/', '')]) + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option1: Left Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        page_url = 'UNKN'\n",
    "        # lookup URL\n",
    "        if page_id in self.URLs:\n",
    "            page_url = self.URLs[page_id]\n",
    "        \n",
    "        yield None, '\\t'.join([page_url, page_id, max_visitor])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\n"
     ]
    }
   ],
   "source": [
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv > 5_2_left_join.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option2: Right Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor, reducer_final=self.output_info)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        self.visit_data = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        self.visit_data[page_id] = max_visitor\n",
    "\n",
    "    def output_info(self):\n",
    "        writer = open('5_2_right_join.txt', 'w')\n",
    "        for page_id in self.URLs:\n",
    "            if page_id in self.visit_data:\n",
    "                writer.write('\\t'.join([self.URLs[page_id], page_id, self.visit_data[page_id]]) + '\\n')\n",
    "            else:\n",
    "                writer.write('\\t'.join([self.URLs[page_id], page_id, 'no_visitor_data']) + '\\n')\n",
    "        writer.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\n"
     ]
    }
   ],
   "source": [
    "# right-side join\n",
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option3: Inner Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        page_url = 'UNKN'\n",
    "        # lookup URL\n",
    "        if page_id in self.URLs:\n",
    "            page_url = self.URLs[page_id]\n",
    "        \n",
    "        if page_url != 'UNKN':\n",
    "            yield None, '\\t'.join([page_url, page_id, max_visitor])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\n"
     ]
    }
   ],
   "source": [
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv > 5_2_inner_join.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Number of rows in left join results:\n",
      "285\n",
      "--------------------------------------\n",
      "Number of rows in right join results:\n",
      "294\n",
      "--------------------------------------\n",
      "Number of rows in inner join results:\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "# report number of rows in the results:\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in left join results:\"\n",
    "print sum(1 for line in open('5_2_left_join.txt', 'r'))\n",
    "\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in right join results:\"\n",
    "print sum(1 for line in open('5_2_right_join.txt', 'r'))\n",
    "\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in inner join results:\"\n",
    "print sum(1 for line in open('5_2_inner_join.txt', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3 For the remainder of this assignment you will work with two datasets:\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "<pre>\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "</pre>\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n",
    "\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longest_5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_5gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# Q1: looking for longest 5-gram (number of characters)\n",
    "\n",
    "class longest_5gram(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        ngram = line.strip().split('\\t')[0]\n",
    "        # stream everyghint into one reducer\n",
    "        yield None, ngram\n",
    "        \n",
    "    def reducer(self, _, ngram):\n",
    "        # since there is only one reducer (only one key), store the logest ngram in memory\n",
    "        self.logest_ngram = None\n",
    "        for ng in ngram:\n",
    "            if self.logest_ngram == None or len(ng) > len(self.logest_ngram):\n",
    "                self.logest_ngram = ng\n",
    "        # emit the finding\n",
    "        yield self.logest_ngram, len(self.logest_ngram)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    longest_5gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"distinct categories of posttransplantation lymphoproliferative\"\t62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-mapper_part-00000\n",
      "writing to c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-mapper_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-mapper_part-00000' 'c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-mapper_part-00001'\n",
      "Piping files into sort for Windows compatibility\n",
      "> sort\n",
      "writing to c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\step-0-reducer_part-00000 -> c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\\output\n",
      "removing tmp directory c:\\temp\\longest_5gram.kuanlin.20160215.054429.573000\n"
     ]
    }
   ],
   "source": [
    "# this should give us the logest ngram\n",
    "!python longest_5gram.py filtered-5Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_vocabs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_vocabs.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script is to find the top 1000 vocabs words\n",
    "\n",
    "class top_vocabs(MRJob):\n",
    "    # use raw value protocol to get inversion done easier\n",
    "    #INPUT_PROTOCOL = RawValueProtocol\n",
    "    #INTERNAL_PROTOCOL = RawValueProtocol\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    # step1 do word counts\n",
    "    # step2 get the top 1000 vocabs by counts\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.wordCountsMapper, reducer=self.wordCountsReducer),\n",
    "            MRStep(reducer_init=self.rankWords_init, reducer=self.getTopVocabs,\n",
    "                  jobconf={\n",
    "                    'stream.num.map.output.key.fields': 2,\n",
    "                    'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': \"-k2,2nr\",\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]      \n",
    "\n",
    "    def wordCountsMapper(self, _, line):\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        ngram_count = int(lineArr[1])\n",
    "        for word in lineArr[0].split(' '):\n",
    "            # each word appear n times as the ngram count\n",
    "            yield word.lower() , ngram_count\n",
    "            \n",
    "    def wordCountsReducer(self, word, counts):\n",
    "        # summing up counts for each word\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def rankWords_init(self):\n",
    "        self.vocab = set()\n",
    "        # grab the top 1000 words as vocab\n",
    "        self.vocab_size = 1000\n",
    "        \n",
    "    def getTopVocabs(self, word, count):\n",
    "        if len(self.vocab) < self.vocab_size:\n",
    "            self.vocab.add(word)\n",
    "            yield word, max(count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    top_vocabs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look for unigram frequencies\n",
    "!python top_vocabs.py filtered-5Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 most frequent words\n",
      "\"the\"\t5490815394\n",
      "\"of\"\t3698583299\n",
      "\"to\"\t2227866570\n",
      "\"in\"\t1421312776\n",
      "\"a\"\t1361123022\n",
      "\"and\"\t1149577477\n",
      "\"that\"\t802921147\n",
      "\"is\"\t758328796\n",
      "\"be\"\t688707130\n",
      "\"as\"\t492170314\n"
     ]
    }
   ],
   "source": [
    "print \"top 10 most frequent words\"\n",
    "num = 0\n",
    "for line in open('vocabs_full.txt', 'r'):\n",
    "    if num < 10: num += 1; print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_density.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# Q3: calculating word densities\n",
    "\n",
    "class word_density(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        (ngram, count, page_count, book_count) = line.strip().split('\\t')\n",
    "        # emit grouped by words\n",
    "        for word in ngram.split(' '):\n",
    "            yield word.strip().lower(), (int(count), int(page_count))\n",
    "        \n",
    "    def reducer(self, word, count_data):\n",
    "        count = 0\n",
    "        page_count = 0\n",
    "        # aggregate counts\n",
    "        for data in count_data:\n",
    "            count += data[0]\n",
    "            page_count += data[1]\n",
    "        yield word, float(count)/float(page_count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_density.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\word_density.kuanlin.20160215.061325.840000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-mapper_part-00000\n",
      "writing to c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-mapper_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-mapper_part-00000' 'c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-mapper_part-00001'\n",
      "Piping files into sort for Windows compatibility\n",
      "> sort\n",
      "writing to c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\word_density.kuanlin.20160215.061325.840000\\step-0-reducer_part-00000 -> c:\\temp\\word_density.kuanlin.20160215.061325.840000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\word_density.kuanlin.20160215.061325.840000\\output\n",
      "removing tmp directory c:\\temp\\word_density.kuanlin.20160215.061325.840000\n"
     ]
    }
   ],
   "source": [
    "!python word_density.py filtered-5Grams > word_density.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most densely appearing words:\n",
      "3.57692307692: pfeffermann\n",
      "3.57692307692: madarassy\n",
      "3.52764423077: cs\n",
      "2.81918819188: irc\n",
      "1.98152424942: chlorinated\n",
      "1.88: electrocardiograph\n",
      "1.84325396825: recalcitrant\n",
      "1.81181485993: rectory\n",
      "1.80952380952: andrea\n",
      "1.80710172745: bailee\n",
      "1.77213240955: unclean\n",
      "1.75384615385: tangibility\n",
      "1.73913043478: nqong\n",
      "1.71653543307: almagro\n",
      "1.71428571429: seacoast\n",
      "1.67346938776: bartas\n",
      "1.6043956044: tra\n",
      "1.60135135135: compellable\n",
      "1.56818181818: echinoidea\n",
      "1.55555555556: demokratischen\n",
      "\n",
      "20 least densely appearing words:\n",
      "1.0: abd\n",
      "1.0: abc\n",
      "1.0: abbotshall\n",
      "1.0: abbotsford\n",
      "1.0: abbassides\n",
      "1.0: abbaside\n",
      "1.0: abbasid\n",
      "1.0: abbacies\n",
      "1.0: abating\n",
      "1.0: abated\n",
      "1.0: abate\n",
      "1.0: abashless\n",
      "1.0: abashed\n",
      "1.0: abased\n",
      "1.0: abandons\n",
      "1.0: abandoning\n",
      "1.0: aback\n",
      "1.0: aazaz\n",
      "1.0: aar\n",
      "1.0: aachen\n"
     ]
    }
   ],
   "source": [
    "word_densities = sorted([(float(line.strip().split('\\t')[1]), line.strip().split('\\t')[0].replace('\"', '')) for line in open('word_density.txt', 'r')], reverse=True)\n",
    "print \"20 most densely appearing words:\"\n",
    "for w in word_densities[:20]:\n",
    "    print \"%s: %s\" %w\n",
    "print\n",
    "print \"20 least densely appearing words:\"\n",
    "for w in word_densities[len(word_densities)-20:]:\n",
    "    print \"%s: %s\" %w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngram_histogram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngram_histogram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# Q4: make ngram char length histogram\n",
    "\n",
    "class ngram_histogram(MRJob):\n",
    "    # don't need key to make histogram\n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        ngram = line.strip().split('\\t')[0]\n",
    "        # just need to print out ngram char length.  no need for reducer\n",
    "        yield None, len(ngram)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ngram_histogram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\step-0-mapper_part-00000\n",
      "writing to c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\step-0-mapper_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\step-0-mapper_part-00000 -> c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\output\\part-00000\n",
      "Moving c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\step-0-mapper_part-00001 -> c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\output\\part-00001\n",
      "Streaming final output from c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\\output\n",
      "removing tmp directory c:\\temp\\ngram_histogram.kuanlin.20160215.063138.378000\n"
     ]
    }
   ],
   "source": [
    "!python ngram_histogram.py filtered-5Grams > char_length_hist_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEZCAYAAACuIuMVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x50fe2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# make the char-length distribution histogram\n",
    "\n",
    "plt.hist([int(l.strip()) for l in open('char_length_hist_data.txt', 'r')])\n",
    "plt.title(\"5-gram char length distribution\")\n",
    "plt.xlabel(\"5-gram char length\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4  (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configuration for running MRJob on EMR\n",
    "import os\n",
    "\n",
    "aws_id = os.environ['AWSAccessKeyId']\n",
    "aws_key = os.environ['AWSSecretKey']\n",
    "\n",
    "writer = open('mrjob_emr.conf', 'w')\n",
    "writer.write(\n",
    "\"\"\"runners:\n",
    "  emr:\n",
    "    aws_access_key_id: %s\n",
    "    aws_region: us-east-1\n",
    "    ec2_instance_type: m1.medium\n",
    "    ec2_master_instance_type: m1.medium\n",
    "    num_ec2_instances: 4\n",
    "    aws_secret_access_key: %s\"\"\"%(aws_id, aws_key)\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_vocabs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_vocabs.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script is to find the top 1000 vocabs words\n",
    "\n",
    "class top_vocabs(MRJob):\n",
    "    # use raw value protocol to get inversion done easier\n",
    "    #INPUT_PROTOCOL = RawValueProtocol\n",
    "    #INTERNAL_PROTOCOL = RawValueProtocol\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    # step1 do word counts\n",
    "    # step2 get the top 10000 vocabs by counts\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.setStopWords, mapper=self.wordCountsMapper, reducer=self.wordCountsReducer),\n",
    "            MRStep(reducer_init=self.rankWords_init, reducer=self.getTopVocabs,\n",
    "                  jobconf={\n",
    "                    'stream.num.map.output.key.fields': 2,\n",
    "                    'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': \"-k2,2nr\",\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]\n",
    "    \n",
    "    def setStopWords(self):\n",
    "        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "                    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "                    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "                    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "                    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "                    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "                    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "                    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "                    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "                    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "                    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "    def wordCountsMapper(self, _, line):\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        ngram_count = int(lineArr[1])\n",
    "        for word in lineArr[0].split(' '):\n",
    "            # each word appear n times as the ngram count\n",
    "            if word.lower() not in self.stop_words:\n",
    "                yield word.lower() , ngram_count\n",
    "            \n",
    "    def wordCountsReducer(self, word, counts):\n",
    "        # summing up counts for each word\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def rankWords_init(self):\n",
    "        self.vocab = set()\n",
    "        # grab the top 10000 words as vocab\n",
    "        self.vocab_size = 10000\n",
    "        \n",
    "    def getTopVocabs(self, word, count):\n",
    "        if len(self.vocab) < self.vocab_size:\n",
    "            self.vocab.add(word)\n",
    "            yield word, max(count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    top_vocabs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python top_vocabs.py -r emr -c mrjob_emr.conf s3://filtered-5grams/ --output-dir=s3://lin.berkeley.mids/w261/hw5/vocab/ --no-output\n",
    "#!python top_vocabs.py googlebooks-eng-all-5gram-20090715-0-filtered.txt --output-dir=vocab --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_matrix.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_matrix.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script creates co-occurence matrix and output into an inverted index\n",
    "\n",
    "class synonym_matrix(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        # first step is to count the frequencies of co-occurence terms\n",
    "        # second step is to produce inverted indexes\n",
    "        return [\n",
    "            MRStep(mapper_init=self.bigram_mapper_init, mapper=self.bigram_mapper, reducer=self.bigram_reducer,\n",
    "                  jobconf={\n",
    "                    \"mapred.map.tasks\":28,\n",
    "                    \"mapred.reduce.tasks\":28}),\n",
    "            MRStep(reducer=self.createInvertedIndex)\n",
    "        ]\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def bigram_mapper_init(self):\n",
    "        self.vocab = set()\n",
    "        self.terms = set()\n",
    "        idx = 0\n",
    "        for line in open('vocabs_no_stop_words.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            word = lineArr[0].replace('\"', '').lower()\n",
    "            self.vocab.add(word)\n",
    "            # bottom 1000 terms are vocabularies\n",
    "            if idx >= 9000:\n",
    "            #if idx >= 500:\n",
    "                self.terms.add(word)\n",
    "            idx += 1\n",
    "    \n",
    "    # count up co-occuring bigrams\n",
    "    def bigram_mapper(self, _, line):\n",
    "        emitted_words = set()\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        words = [w.lower() for w in lineArr[0].split(' ')]\n",
    "        # total counts of this particular ngram\n",
    "        ngram_counts = int(lineArr[1])\n",
    "        if len(words) > 1:\n",
    "            for word in words:\n",
    "                if word not in self.vocab or word in emitted_words:\n",
    "                    continue\n",
    "                # emit total ngram count to calculate frequency in the reducer\n",
    "                yield word, {'all': ngram_counts}\n",
    "                # stripe for the current word\n",
    "                word_stripe = {}\n",
    "                emitted_words.add(word)\n",
    "                co_occured_words = set()\n",
    "                for another_word in words:\n",
    "                    if word != another_word and another_word in self.terms and another_word not in co_occured_words:\n",
    "                        co_occured_words.add(another_word)\n",
    "                        word_stripe[another_word] = ngram_counts\n",
    "                yield word, word_stripe\n",
    "    \n",
    "    # calculating co-occuring frequencies\n",
    "    def bigram_reducer(self, word, stripes):       \n",
    "        ngram_total = 0\n",
    "        current_stripe = {}\n",
    "        for stripe in stripes:\n",
    "            if len(stripe) == 1 and 'all' in stripe:\n",
    "                ngram_total += stripe['all']\n",
    "            else:\n",
    "                # element-wise sum up co-occurences\n",
    "                for coword in stripe:\n",
    "                    if coword not in current_stripe:\n",
    "                        current_stripe[coword] = stripe[coword]\n",
    "                    else:\n",
    "                        current_stripe[coword] += stripe[coword]\n",
    "        if ngram_total > 0:\n",
    "            for coword in current_stripe:\n",
    "                yield coword, (word, float(current_stripe[coword])/float(ngram_total))\n",
    "    \n",
    "    # make the inverted index\n",
    "    def createInvertedIndex(self, cowords, word_freq):\n",
    "        yield cowords, '|'.join([\"%s:%s\"%(word, freq) for (word, freq) in word_freq])\n",
    "     \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    synonym_matrix.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python synonym_matrix.py -r emr -c mrjob_emr.conf s3://filtered-5grams/ --file vocabs_no_stop_words.txt --output-dir=s3://lin.berkeley.mids/w261/hw5/syn_matrix/ --no-output\n",
    "#!python synonym_matrix.py googlebooks-eng-all-5gram-20090715-0-filtered.txt --file vocabs_no_stop_words.txt --output-dir=bigram_results --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_detection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_detection.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# synonym detection with cosine similarity and jaccard index\n",
    "\n",
    "class synonym_detection(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        # define configurations to use more map and reduce tasks\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init, mapper=self.mapper, reducer=self.reducer,\n",
    "                  jobconf={\n",
    "                    \"mapred.map.tasks\":28,\n",
    "                    \"mapred.reduce.tasks\":28})\n",
    "        ]\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def mapper_init(self):\n",
    "        self.vocab = []\n",
    "        for line in open('vocabs_no_stop_words.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.append(lineArr[0].replace('\"', '').lower())\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().replace('\"', '').split('\\t')\n",
    "        co_word = lineArr[0]\n",
    "        doc_terms = {}\n",
    "        # parse the stripes of co-occuring terms\n",
    "        for term_freq in lineArr[1].split('|'):\n",
    "            term = term_freq.split(':')[0]\n",
    "            freq = float(term_freq.split(':')[1])\n",
    "            doc_terms[term] = freq\n",
    "        \n",
    "        # emits components of term co-occurence frequencies\n",
    "        for i, term1 in enumerate(self.vocab):\n",
    "            term_stripe = {}\n",
    "            if i != len(self.vocab)-1:\n",
    "                for term2 in self.vocab[i:]:\n",
    "                    if term1 == term2:\n",
    "                        continue\n",
    "                    term1_freq = 0.0\n",
    "                    term2_freq = 0.0\n",
    "                    # co-occuring frequencies for each term\n",
    "                    if term1 in doc_terms:\n",
    "                        term1_freq = doc_terms[term1]\n",
    "                    if term2 in doc_terms:\n",
    "                        term2_freq = doc_terms[term2]\n",
    "                    # collect data into co-occuring terms\n",
    "                    if term1_freq > 0 or term2_freq > 0:\n",
    "                        term_stripe[term2] = (term1_freq, term2_freq)\n",
    "                yield term1, term_stripe\n",
    "    \n",
    "    # cosine similarity calculation\n",
    "    def calcCosineSim(self, values):\n",
    "        a_b = 0.0\n",
    "        a_sqr = 0.0\n",
    "        b_sqr = 0.0\n",
    "        for val in values:\n",
    "            a_sqr += val[0]**2\n",
    "            b_sqr += val[1]**2\n",
    "            a_b += val[0]*val[1]\n",
    "            \n",
    "        if a_sqr == 0 or b_sqr == 0:\n",
    "            return  0.0\n",
    "        else:\n",
    "            return a_b/((a_sqr**0.5)*(b_sqr**0.5))\n",
    "    \n",
    "    # jacard index calculation\n",
    "    def jaccardSim(self, values):\n",
    "        a_sum = 0\n",
    "        b_sum = 0\n",
    "        a_inter_b = 0\n",
    "        for val in values:\n",
    "            if val[0] > 0:\n",
    "                a_sum += 1\n",
    "            if val[1] > 0:\n",
    "                b_sum += 1\n",
    "            if val[0] > 0 and val[1] > 0:\n",
    "                a_inter_b += 1\n",
    "        if (a_sum + b_sum - a_inter_b) == 0:\n",
    "            return 0.0\n",
    "        return float(a_inter_b)/float(a_sum + b_sum - a_inter_b)\n",
    "    \n",
    "    # calculates the overall cosine similarity\n",
    "    def reducer(self, term1, term2_vals):\n",
    "        term2_freqs = {}\n",
    "        for val in term2_vals:\n",
    "            for term in val:\n",
    "                if term not in term2_freqs:\n",
    "                    term2_freqs[term] = [val[term]]\n",
    "                else:\n",
    "                    term2_freqs[term].append(val[term])\n",
    "        \n",
    "        for term2 in term2_freqs:\n",
    "            cosine_sim = self.calcCosineSim(term2_freqs[term2])\n",
    "            jaccard_sim = self.jaccardSim(term2_freqs[term2])\n",
    "            # only print out similar iterms\n",
    "            if cosine_sim > 0.5 or jaccard_sim > 0.5:\n",
    "                yield term1+':'+term2, (cosine_sim, jaccard_sim)\n",
    "        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    synonym_detection.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python synonym_detection.py -r emr -c mrjob_emr.conf s3://lin.berkeley.mids/w261/hw5/syn_matrix/ --file vocabs_no_stop_words.txt --output-dir=s3://lin.berkeley.mids/w261/hw5/syn_detection/ --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 5.5 \n",
    "Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the top 1000 most similar pairs\n",
    "\n",
    "def topNbuffer(orig_list, to_add, max_size):\n",
    "    orig_list.append(to_add)\n",
    "    if len(orig_list) > max_size:\n",
    "        return sorted(orig_list)[len(orig_list)-max_size:]\n",
    "    else:\n",
    "        return orig_list\n",
    "\n",
    "# iterate through items and keep the top 1000\n",
    "import os\n",
    "top_items = []\n",
    "for f in os.listdir('synon_result'):\n",
    "    for line in open(os.path.join('synon_result', f), 'r'):\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        word_pair = lineArr[0].replace('\"', '')\n",
    "        sim_score = lineArr[1].split(',')[0].strip().replace('[', '')\n",
    "        top_items = topNbuffer(top_items, (sim_score, word_pair), 1000)\n",
    "        \n",
    "writer = open('topSimilarPairs.txt', 'w')\n",
    "for item in sorted(top_items, reverse=True):\n",
    "    writer.write(\"%s\\t%s\"%item + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98014031371310784\tstates:nations\n",
      "0.96507070656216787\tseem:appear\n",
      "0.94469669604509343\tten:twenty\n",
      "0.92882170590490287\ttoward:towards\n",
      "0.91405295026639233\tpresident:constitution\n",
      "0.90898296619614394\tmatter:spite\n",
      "0.90786649425709975\twant:wanted\n",
      "0.90407927164842428\trange:variety\n",
      "0.89982751577342557\tstates:kingdom\n",
      "0.89711671182029207\tattempt:effort\n"
     ]
    }
   ],
   "source": [
    "# a peak of the top 10 items\n",
    "num = 0\n",
    "for line in open('topSimilarPairs.txt', 'r'):\n",
    "    if num < 10: print line.strip()\n",
    "    else: break\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.086\n",
      "recall: 0.0111053719008\n",
      "f1 score: 0.01967063129\n",
      "macro average: 0.038925334397\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall, and F1 score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "fn_pool = {}\n",
    "tp = 0\n",
    "for line in open('topSimilarPairs.txt', 'r'):\n",
    "    term1, term2 = line.strip().split('\\t')[1].split(':')\n",
    "    #synonyms identified by wordnet\n",
    "    term1_syns = [t.lower() for t in synonyms(term1)]\n",
    "    term2_syns = [t.lower() for t in synonyms(term2)]\n",
    "    \n",
    "    # identify true positive\n",
    "    if term2 in term1_syns or term1 in term2_syns:\n",
    "        tp += 1\n",
    "        \n",
    "    # keep a list of false negatives\n",
    "    if term1 not in fn_pool:\n",
    "        fn_pool[term1] = set()\n",
    "        for t in term1_syns:\n",
    "            if t != term2:\n",
    "                fn_pool[term1].add(t)\n",
    "    else:\n",
    "        # if true positive, remove from false negative list\n",
    "        if term2 in fn_pool[term1]:\n",
    "            fn_pool[term1].remove(term2)\n",
    "            \n",
    "    # do the same for term2\n",
    "    if term2 not in fn_pool:\n",
    "        fn_pool[term2] = set()\n",
    "        for t in term2_syns:\n",
    "            if t != term1:\n",
    "                fn_pool[term2].add(t)\n",
    "    else:\n",
    "        # if true positive, remove from false negative list\n",
    "        if term1 in fn_pool[term2]:\n",
    "            fn_pool[term2].remove(term1)\n",
    "    \n",
    "# As stated in the assignment, true possitive is when either of the word is the synonym of the other as identified by wordnet.\n",
    "# Therefore false positive is any pair in the 1000 pairs that failed the aforementioned test.\n",
    "# For false negatives, we are going with the definition on Live-Lecture 6 page 46, which states that\n",
    "# any word pairs identifed by the wordnet but not identified by our algorithm counts as false negative\n",
    "\n",
    "fn = 0\n",
    "# sum up total false negatives\n",
    "for term in fn_pool:\n",
    "    fn += len(fn_pool[term])\n",
    "\n",
    "precision = tp/1000.0\n",
    "recall = float(tp)/(tp + fn)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print \"precision: %s\"%precision\n",
    "print \"recall: %s\"%recall\n",
    "print \"f1 score: %s\"%f1_score\n",
    "print \"macro average: %s\"%((precision+recall+f1_score)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
