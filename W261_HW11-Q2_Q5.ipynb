{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Jan 29 2016 14:26:21)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#spark_home = os.environ['SPARK_HOME'] = 'C:\\\\spark-1.5.1-bin-hadoop2.4'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\spark-1.6.1-bin-hadoop2.6'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "#sys.path.insert(0,os.path.join(spark_home,'python', 'lib', 'py4j-0.8.2.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python', 'lib', 'py4j-0.9-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python', 'pyspark', 'shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW11.2 Gradient descent\n",
    "\n",
    "In the context of logistic regression describe and define three flavors of penalized loss functions.  \n",
    "Are these all supported in Spark MLLib (include online references to support your answers)?\n",
    "\n",
    "Descibe probabilitic interpretations of the L1 and L2 priors for penalized logistic regression\n",
    "(HINT: see synchronous slides for week 11 for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of logistic regression, the three flavors of penalized terms are:\n",
    "- L1 Reg, which penalizes for sum of absolute weights:\n",
    "![alt text](http://people.ischool.berkeley.edu/~kuanlin/l1_reg.PNG \"L1 Reg\")\n",
    "- L2 Reg, penalizes sum of squared weights:\n",
    "![alt text](http://people.ischool.berkeley.edu/~kuanlin/l2_reg.PNG \"L2 Reg\")\n",
    "- Elastic Net, penalizes a linear combination of L1 and L2 norms:\n",
    "![alt text](http://people.ischool.berkeley.edu/~kuanlin/elastic_net.PNG \"Elastic Net Reg\")\n",
    "\n",
    "All of the above three regularization methods are supported by spark.mllib:<br/>\n",
    "http://spark.apache.org/docs/latest/mllib-linear-methods.html#regularizers\n",
    "\n",
    "#### Probablisitic interpretation of L1 and L2 priors:\n",
    "\n",
    "L1 regularization can be interpreted as using Laplace distribution as the prior distribution for the model weights, where as L2 regularization can be interpreted as using gaussian distribution as the prior distribution for the model weights.\n",
    "![alt text](http://people.ischool.berkeley.edu/~kuanlin/gaussian_vs_laplace.PNG \"Distribution Overlay\")\n",
    "\n",
    "The Laplace distribution has more density closer to mean (usually zero in most settings) in comparison to the gaussian distribution, and therefore L1 regularization will tend to push the model weights toward zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "def generateData2(n):\n",
    "    \"\"\" \n",
    "    non-linearly seperable data\n",
    "    \"\"\"\n",
    "    xb = np.random.normal(0,0.5,n)-0.5\n",
    "    yb = np.random.normal(0,0.5,n)+0.5\n",
    "    xr = np.random.normal(0,0.5,n)+0.5\n",
    "    yr = np.random.normal(0,0.5,n)-0.5\n",
    "    inputs = []\n",
    "    for i in range(len(xb)):\n",
    "        inputs.append([xb[i],yb[i],1])\n",
    "        inputs.append([xr[i],yr[i],-1])\n",
    "    return inputs\n",
    "\n",
    "data_lin_inseperable_train = generateData2(50) # train data\n",
    "data_lin_inseperable_test = generateData2(50) # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.5  [OPTIONAL] Distributed Perceptron algorithm.\n",
    "\n",
    "Using the following papers as background:\n",
    "http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36266.pdf\n",
    "\n",
    "https://www.dropbox.com/s/a5pdcp0r8ptudgj/gesmundo-tomeh-eacl-2012.pdf?dl=0\n",
    "\n",
    "http://www.slideshare.net/matsubaray/distributed-perceptron \n",
    "\n",
    "Implement each of the following flavors of perceptron learning algorithm:\n",
    "\n",
    "1. Serial (All Data): This is the classifier returned if trained serially on all the \n",
    "available data.  On a single computer for example (Mistake driven)\n",
    "2. Serial (Sub Sampling): Shard the data, select one shard randomly and train serially. \n",
    "3. Parallel (Parameter Mix): Learn a perceptron locally on each shard: \n",
    "Once learning is complete combine each learnt percepton using a uniform weighting\n",
    "4. Parallel (Iterative Parameter Mix) as described in the above papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptronSubGradientCalc(data, w, regParam, feature_size):\n",
    "    #gradient = data.map(lambda p: -p.y*np.array(p.x) if (p.y*np.dot(w.value,p.x))<0 else np.zeros(feature_size)).reduce(lambda a, b: a+b)\n",
    "    incorrect = data.filter(lambda p: np.dot(w.value,p.x)*p.y < 0)\n",
    "    if incorrect.isEmpty():\n",
    "        gradient = np.zeros(feature_size)\n",
    "    else:\n",
    "        gradient = incorrect.map(lambda p: -p.y*np.array(p.x)).reduce(lambda a,b: a+b)\n",
    "    wreg = np.array(w.value)  # L2 reg\n",
    "    wreg[-1] = 0 # don't count the bias term in reg\n",
    "    return gradient/data.count() + regParam*wreg\n",
    "\n",
    "def perceptronAccuracy(data, w):\n",
    "    correct_count = data.map(lambda p: 1 if np.dot(w.value, p.x)*p.y > 0 else 0).reduce(lambda a,b: a+b)\n",
    "    return 1.0*correct_count/data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "Point = namedtuple('Point', 'x y')\n",
    "\n",
    "def readPoint(data):\n",
    "    label = data[2]\n",
    "    x = [data[0], data[1], 1.0]  #add bias term\n",
    "    return Point(x, label)\n",
    "\n",
    "train_data = sc.parallelize(data_lin_inseperable_train).map(readPoint).cache()\n",
    "test_data = sc.parallelize(data_lin_inseperable_test).map(readPoint).cache()\n",
    "\n",
    "#train_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train serially on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weight: [-2.16751633  0.22811287 -0.02710354]\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "def perceptronGDSerialAll(data, wInitial=None, nShards=1, nIter=10, stopCriteria=0.001, \n",
    "                 learningRate=0.05, regParam=0.01):\n",
    "    feature_size = len(data.take(1)[0].x)\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=feature_size)\n",
    "    else:\n",
    "        w = wInitial\n",
    "    #model_data = data.randomSplit([1.0/nShards]*nShards) # split data into shards\n",
    "    for i in range(nIter):\n",
    "        wb = sc.broadcast(w) # broadcast weight\n",
    "        wdelta = learningRate*perceptronSubGradientCalc(data, wb, regParam, feature_size)\n",
    "        if sum(abs(wdelta))<=stopCriteria*sum(abs(w)):\n",
    "            break\n",
    "        w = w - wdelta\n",
    "    return w\n",
    "\n",
    "w = perceptronGDSerialAll(train_data)\n",
    "print \"learned weight: %s\" % str(w)\n",
    "print \"Accuracy: %s\" % perceptronAccuracy(test_data, sc.broadcast(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train serially on random shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weight: [-0.0394292   0.17467005 -0.00799986]\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "def perceptronGDSerialShards(data, wInitial=None, nShards=1, nIter=10, stopCriteria=0.001, \n",
    "                 learningRate=0.05, regParam=0.01):\n",
    "    feature_size = len(data.take(1)[0].x)\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=feature_size)\n",
    "    else:\n",
    "        w = wInitial\n",
    "    for shared_data in data.randomSplit([1.0/nShards]*nShards): # split data into shards\n",
    "        for i in range(nIter):\n",
    "            wb = sc.broadcast(w) # broadcast weight\n",
    "            wdelta = learningRate*perceptronSubGradientCalc(shared_data, wb, regParam, feature_size)\n",
    "            if sum(abs(wdelta))<=stopCriteria*sum(abs(w)):\n",
    "                break\n",
    "            w = w - wdelta\n",
    "    return w\n",
    "\n",
    "w = perceptronGDSerialShards(train_data, nShards=4, nIter=50)\n",
    "print \"learned weight: %s\" % str(w)\n",
    "print \"Accuracy: %s\" % perceptronAccuracy(test_data, sc.broadcast(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Mixing Method (non-iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weight: [ 0.82557121  2.61303986 -0.09620172]\n",
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "def percetronTrainLocal(data, w, regParam, feature_size, learningRate, nIter):\n",
    "    # train with local data instead of RDD\n",
    "    weight = np.array(w.value)\n",
    "    for i in range(nIter):\n",
    "        gradient = sum(map(lambda p: -p.y*np.array(p.x) if (p.y*np.dot(weight,p.x))<0 else np.zeros(feature_size), data))\n",
    "        wreg = weight*1  # L2 reg\n",
    "        wreg[-1] = 0 # don't count the bias term in reg\n",
    "        weight -= learningRate*(gradient/len(data) + regParam*wreg)\n",
    "    return weight\n",
    "\n",
    "def perceptronGDSerialParaMix(data, wInitial=None, nShards=1, nIter=10, learningRate=0.05, regParam=0.01):\n",
    "    feature_size = len(data.take(1)[0].x)\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=feature_size)\n",
    "    else:\n",
    "        w = wInitial\n",
    "    wb = sc.broadcast(w)\n",
    "    model_data = sc.parallelize([d.collect() for d in data.randomSplit([1.0/nShards]*nShards)]) # split data into shards in RDD\n",
    "    learned_w = model_data.map(lambda data: percetronTrainLocal(data, wb, regParam, feature_size, learningRate, nIter)).collect()\n",
    "    return sum(learned_w)/len(learned_w) # take uniform average of the learned weights\n",
    "\n",
    "w = perceptronGDSerialParaMix(train_data, nShards=4, nIter=50)\n",
    "print \"learned weight: %s\" % str(w)\n",
    "print \"Accuracy: %s\" % perceptronAccuracy(test_data, sc.broadcast(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Param Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weight: [-1.16103408  0.03655426  0.44536896]\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "def perceptronGDSerialIterParaMix(data, wInitial=None, nShards=1, nIter=10, learningRate=0.05, regParam=0.01):\n",
    "    feature_size = len(data.take(1)[0].x)\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=feature_size)\n",
    "    else:\n",
    "        w = wInitial\n",
    "    model_data = sc.parallelize([d.collect() for d in data.randomSplit([1.0/nShards]*nShards)]) # split data into shards in RDD\n",
    "    for i in range(nIter):\n",
    "        wb = sc.broadcast(w)\n",
    "        # train only 1 epoch and then do param mixing\n",
    "        learned_w = model_data.map(lambda data: percetronTrainLocal(data, wb, regParam, feature_size, learningRate, 1)).collect()\n",
    "        w = sum(learned_w)/len(learned_w) # take uniform average of the learned weights\n",
    "    return w\n",
    "\n",
    "w = perceptronGDSerialIterParaMix(train_data, nShards=4, nIter=50)\n",
    "print \"learned weight: %s\" % str(w)\n",
    "print \"Accuracy: %s\" % perceptronAccuracy(test_data, sc.broadcast(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
