{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS UC Berkeley, Machine Learning at Scale DATSCI W261 ASSIGNMENT 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 13.1: Spark implementation of basic PageRank\n",
    "\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input.\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number \n",
    "of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), \n",
    "starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code \n",
    "is as efficient as possible.\n",
    "\n",
    "\n",
    "As you build your code, use the test data\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), \n",
    "and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "<pre>\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "</pre>\n",
    "\n",
    "Run this experiment locally first. Report the local configuration that you\n",
    " used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "Repeat this experiment on AWS. Report the AWS cluster configuration that \n",
    "you used and how long in minutes and seconds it takes to complete your job. \n",
    "(in your notebook, cat the cluster config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Jan 29 2016 14:26:21)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\spark-1.6.1-bin-hadoop2.6'\n",
    "#spark_home = os.environ['SPARK_HOME'] = '/home/w205/spark15'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python', 'lib', 'py4j-0.9-src.zip'))\n",
    "#sys.path.insert(0,os.path.join(spark_home,'python', 'lib', 'py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python', 'pyspark', 'shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n",
      "Node: B | Rank: 0.363235948989\n",
      "Node: C | Rank: 0.362883728039\n",
      "Node: E | Rank: 0.0811452576255\n",
      "Node: D | Rank: 0.03938466342\n",
      "Node: F | Rank: 0.03938466342\n",
      "Node: A | Rank: 0.0329301017862\n",
      "Node: I | Rank: 0.0162071273441\n",
      "Node: H | Rank: 0.0162071273441\n",
      "Node: K | Rank: 0.0162071273441\n",
      "Node: G | Rank: 0.0162071273441\n",
      "Node: J | Rank: 0.0162071273441\n"
     ]
    }
   ],
   "source": [
    "input_file = 'PageRank-test.txt'\n",
    "iter_num = 10\n",
    "tele_factor = 0.15\n",
    "\n",
    "def parseInput(line):\n",
    "    lineArr = line.split('\\t')\n",
    "    key_url = lineArr[0].strip()\n",
    "    neighbors = eval(lineArr[1])\n",
    "    return (key_url, neighbors)\n",
    "\n",
    "def calcContribution(neighbors, rankVal, loss_mass_accumulator):\n",
    "    if neighbors != None:\n",
    "        link_total_weight = sum(neighbors.values())\n",
    "        for link in neighbors:\n",
    "            link_weight = neighbors[link]\n",
    "            yield link, rankVal*1.0*link_weight/link_total_weight\n",
    "    else:\n",
    "        loss_mass_accumulator.add(rankVal)\n",
    "        \n",
    "def reNormalizeRank(iter_result, loss_mass, total_nodes, damping_factor):\n",
    "    if iter_result == None: iter_result = 0.0\n",
    "    return (iter_result+loss_mass)*(1-damping_factor)+damping_factor*1.0/total_nodes\n",
    "\n",
    "# load the sparse input, and un-sparse it\n",
    "adj_rdd = sc.textFile(input_file).map(lambda line: parseInput(line))\n",
    "all_urls = adj_rdd.flatMap(lambda data: [(data[0], [data[1]])] + [(url, [None]) for url in data[1].keys()]) \\\n",
    "        .reduceByKey(lambda a, b: a + b).mapValues(lambda vals: max(vals))\n",
    "\n",
    "total_nodes = all_urls.count()\n",
    "\n",
    "# initialize ranks\n",
    "ranks = all_urls.map(lambda (url,neighbors): (url,(neighbors, 1.0/total_nodes)))\n",
    "\n",
    "for i in range(iter_num):\n",
    "    print \"iter %s\"%i\n",
    "    accum_loss_mass = sc.accumulator(0.0)\n",
    "   \n",
    "    iter_distribution = ranks \\\n",
    "            .flatMap(lambda data: calcContribution(data[1][0], data[1][1], accum_loss_mass)) \\\n",
    "            .reduceByKey(lambda a,b: a+b).cache()\n",
    "    iter_distribution.first() # force RDD to evaluate\n",
    "    loss_mass = accum_loss_mass.value/total_nodes\n",
    "    \n",
    "    ranks = all_urls.leftOuterJoin(iter_distribution) \\\n",
    "        .mapValues(lambda (neighbors, iter_result): (neighbors,reNormalizeRank(iter_result, loss_mass, total_nodes, tele_factor)))\n",
    "\n",
    "# take a look at top 11\n",
    "for result in ranks.takeOrdered(11, lambda (k, v): -v[1]):\n",
    "    print \"Node: %s | Rank: %s\" % (result[0], result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
